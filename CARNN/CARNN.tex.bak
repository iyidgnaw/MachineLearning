% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}

\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{subfigure}
\usepackage{booktabs}


\begin{document}
\title{Context-aware Sequential Recommendation}

\numberofauthors{4}
\author{
        Anonymous CIKM submission\\
        \\
        Paper ID 22
       }
\maketitle


\begin{abstract}
With the rapid growth of Internet applications, sequential prediction in collaborative filtering has become an emerging and crucial task. Given the behavioral history of a specific user, predicting his or her next choice plays a key role for improvement in various online services. Meanwhile, there are more and more scenarios with multiple types of behaviors, while existing works mainly study sequences with a single type of behavior. As a widely used approach, Markov chain based models are constructed on a strong independence assumption. As two classical neural network based methods for modeling sequences, recurrent neural networks can not well model short-term contexts, and log-bilinear model is not suitable for long-term contexts. In this paper, we propose a Recurrent Log-BiLinear (RLBL) model. It models multiple types of behaviors in historical sequences with behavior-specific transition matrices. Meanwhile, RLBL employs position-specific transition matrices for modeling short-term contexts and uses a recurrent structure for modeling long-term contexts. Moreover, conventional sequential models have problem in handling continuous time difference between input elements in behavioral history, which is a key factor for dynamic prediction. Thus, we further extend RLBL via replacing position-specific transition matrices with time-specific transition matrices, and propose Time-Aware Recurrent Log-BiLinear (TA-RLBL). Experimental results show that the proposed CA-RNN model yields significant improvements over the state-of-the-art sequential recommendation methods and context-aware recommendation methods on three typical datasets, i.e., Tmall dataset and Fashion dataset.
\end{abstract}

\keywords{Sequential recommendation, context-awareness, recurrent neural networks} % NOT required for Proceedings

\section{Introduction}
Nowadays, people are overwhelmed by huge amount of information, the exposure to information made people tired of extracting useful and valuable information that they are interested in. This phenomenon turns out to facilitate the development of recommender systems in social networking, e-commerce, online movie and reading websites. Recommender system has now been an important tool for helping people to filter information and locate their preference. Conventional recommendation methods focus on modeling users' preference based on their historical choices of items and always ignore the sequential information. However, user preferences always change with time. Historical behaviors in different time periods have different effects on users' next choice. Accordingly, sequential recommendation is a crucial task for predicting users' next behaviors in recommender sysytems.

Nowadays, the importance of sequential information in recommender system has been gradually recognized by researchers in many disciplines, and some efforts have been put into developing CF methods with sequential information \cite{campos2014time}. Markov Chain (MC) based models \cite{yang2010personalizing,rendle2010factorizing,natarajan2013app,chen2015personalized} have been widely used for sequential prediction. MC based models aim to predict the users' next behavior based on the past behaviors in sequential data. A transition matrix is estimated, which can give the probability of an action based on the previous ones. For personalized recommendation, Factorization Personalized Markov Chain (FPMC) \cite{rendle2010factorizing} provide more accurate prediction by factorizing a personalized transition tensor. However, a major problem of MC based models is that all the components are independently combined, indicating that it makes strong independence assumption among multiple factors. Furthermore, MC based methods have been extended via representation learning, which have been applied to next basket recommendation \cite{wang2015learning}. Recently, as another typical representation learning method, Recurrent Neural Networks (RNN) have been employed to model temporal dependency for different applications successfully. RNN consists of an input layer, an output unit and multiple hidden layers. Hidden representation of RNN can change dynamically along with a sequential history. Each layer of RNN contains an input element and recurrent transition from the previous status, which are captured by an input matrix and a transition matrix respectively. RNN is famous for its being successfully applied in sentence modeling tasks \cite{mikolov2010recurrent,mikolov2011extensions,mikolov2011rnnlm}. It also achieves state-of-the-art performances in sequential click prediction \cite{zhang2014sequential}, location prediction \cite{liu2016strnn} and next basket recommendation \cite{yu2016dream}. Considering its great performances in sequential modeling tasks, RNN is a suitable tool for sequential recommendation.

Though RNN has achieved satisfactory performances, it still has its drawbacks in sequential recommendation situations. Nowadays, with enhanced ability of systems in collecting information, a great amount of contextual information in recommender systems has been collected such as location, time, weather and so on. These kinds of contextual information have significant effect on user behaviors. For instance, a man may like to watch cartoon with his children while may like to watch romantic movies with his wife. He may prefer to read novels during weekend while may tend to read professional books during weekdays. Contextual information has been proved to be useful in determining users' preferences in recommender systems \cite{palmisano2008using,adomavicius2011context}. Context-aware recommendation has been extensively studied and several methods have been proposed to achieve state-of-the-art performances \cite{rendle2011fast,shi2012tfmap,jamali2013heteromf,shi2014cars,liu2015cot}. Without considering the rich contextual information in real world, RNN and other sequential recommendation methods can only model historical sequences and are hard to further improve the performances of recommendation in complex real applications. In contrast, there are also no context-aware recommendation methods that take sequential information into consideration. Unfortunately, as shown in the example in Figure, complex real-world applications usually have both sequential and contextual information. Thus, context-aware recommendation becomes an emerging task, and adapting RNN for variety of contextual information is a good methodology.

To incorporate contextual information in RNN and other sequential models, we investigate the properties of sequential behavioral histories and conclude two types of contexts: \textbf{input contexts} and \textbf{transition contexts}. These two types of contexts are shown in the demonstration in Figure. And we will explain them one by one.

Input contexts denote the contexts of input elements in behavioral sequences, that is to say, input contexts are situations that users conduct behaviors, e.g., shopping, visiting or reading. Such contexts usually include location (home or working place), time (weekdays or weekends, mourning or evening), weather (sunny or rainy) and so on. As we mentioned above, input contexts have significant effects on predicting present behaviors of users. Similarly, input contexts in the history are also useful for predicting the future. For instance, if a man usually takes excise in the morning, then we can predict he will go to the fitting room in the morning rather than evening, even if there are also lots of people going to fitting rooms in the evening.

Transition contexts are the contexts for the transition from previous behaviors in historical sequences. They capture how the previous behaviors affect the future. Specifically, transition contexts denote time intervals between adjacent behaviors in sequences in this paper. Generally speaking, longer time intervals and shorter time intervals means different for the transition from the past. If a user's last behavior on an online shopping website happens half a year ago, then his or her past behaviors have limited effects on the user's future purchasing and a recommendation with popular items may be a better choice. If the user's last behavior happens yesterday, then his or her next purchasing will be significantly affected by the previous ones and a personalized recommendation should be made.

Thus, to model sequential information and contextual information in one framework, we propose a novel model called Context-Aware Recurrent Neural Networks (\textbf{CA-RNN}). Instead of a constant input matrix for capturing input elements in each layer of RNN, we use context-specific input matrices for each specific input contexts. Similarly, we use context-specific transition matrices for modeling the transition effects from previous behaviors in historical sequences under specific transition contexts, i.e., time intervals between adjacent behaviors. Then, we implement our CA-RNN model in a Bayesian Personalized Ranking (BPR) \cite{rendle2009bpr} framework, and Back Propagation Through Time (BPTT) \cite{rumelhart1988learning} is applied for learning parameters of CA-RNN. In summary, the main contributions of this work are listed as follows:

\begin{itemize}
\item
We address the problem of context-aware sequential recommendation, which presents a novel perspective for better recommendation. And we conclude two types of contexts in this problem: input contexts and transition contexts.

\item
The RLBL model incorporates position-specific matrices and the recurrent structure, which can well model both the short- and long-term contexts in historical sequences.

\item
TA-RLBL uses time-specific matrices to jointly model sequential information and time difference information in one framework, which further improves the performance of RLBL.

\item
Experiments conducted on two real-world datasets show that CA-RNN is effective and clearly outperforms the state-of-the-art methods.

\end{itemize}

The rest of the paper is organized as follows. In section 2, we review some related work on sequential recommendation and context-aware recommendation. Then we give the formulation of context-aware sequential recommendation in section 3. Section 4 details our CA-RNN model. In section 5, we introduce the learning methods of our proposed model. In section 6, we conduct experiments in two real-world datasets and compare with several state-of-the-art methods. Section 7 concludes our work and discusses future research.

\section{Related Works}
In this section, we review some related works on sequential recommendation and context-aware recommendation.

\subsection{Sequential Recommendation}

Time-aware neighbourhood models \cite{ding2005time,lathia2009temporal,liu2010online} may be the most natural methods for modeling sequences, which employ neighbourhood based algorithms to capture temporal effects via giving more relevance to recent observations and less to past observations. Using frequent pattern mining, sequential pattern based methods \cite{mobasher2002using,hariri2012context} seek sequential patterns that occur most frequently to predict the future. However, both neighbourhood based and sequential pattern based methods are unable to reveal the underlying properties in users' historical sequences. And sequential pattern based methods are time consuming in large scale datasets.

Matrix factorization based methods \cite{koren2009matrix} have become the state-of-the-art approach to conventional recommendation, which have been extended to time-aware factorization based models recently. Tensor Factorization (TF) \cite{xiong2010temporal,bahadori2014fast} treats time intervals as another dimension and generate latent vectors of time intervals via factorization to capture the underlying properties in historical sequences. TimeSVD++ \cite{koren2010collaborative} learns time-aware representations for users and items. However, factorization based models have difficulties in generating latent representations for time intervals which has never or seldom appeared in the training data.

The MC based methods are widely used models for sequential applications \cite{yang2010personalizing}. Via factorization of the probability transition matrix, Factorizing Personalized Markov Chain (FPMC) \cite{rendle2010factorizing} can provide more accurate prediction for each sequence. FPMC is also extended by using user group \cite{natarajan2013app} or incorporating location constraint \cite{cheng2013you}. Recently, some factors of human brain have been added into MC based methods, including interest-forgetting curve \cite{chen2015personalized} and dynamics of boredom \cite{kapoor2015just}. However, the main drawback of MC based models is the independent combination of the past components, which lies in a strong independence assumption and confines the prediction accuracy. MC based methods are then extended by representation learning. Hierarchical Representation Model (HRM) \cite{wang2015learning} learns the representation of behaviors in the last transaction and predicts behaviors for the next transaction. And Personalized Ranking Metric Embedding (PRME) \cite{feng2015personalized} learns embeddings of users according to the location distance.

Recently, a few prediction models, especially language models, are proposed based on neural networks. The most classical language model is proposed via a single layer neural network \cite{bengio2003neural}. And models based on RNN have been successfully used in modeling sentences \cite{mikolov2010recurrent,mikolov2011extensions,mikolov2011rnnlm}. RNN also brings satisfying results for sequential click prediction for sponsored search \cite{zhang2014sequential}, location prediction \cite{liu2016strnn} and next basket recommendation \cite{yu2016dream}. RNN is showing its great performances in sequential modeling.

\subsection{Context-aware Recommendation}

Context-aware recommender system (CARS) \cite{adomavicius2011context} generates more relevant recommendations by adapting them to the specific contextual situation of the user, and it built based on the knowledge of contextual user preferences and typically deal with data records of user, item and context. Recent works on contextual modeling approaches extend the user-item preference relations with contextual information and incorporate factorization models to compute recommendations. Multi-verse recommendation \cite{karatzoglou2010multiverse} uses tensor factorization to model n-dimensional contextual information. The social network aided context-aware recommender system \cite{liu2013soco} splits contexts and performs general matrix factorization on each leaf of decision trees. As a general factorization model, Factorization Machine (FM) \cite{rendle2011fast} can model a wide variety of contextual information by specifying contextual information as the input dimensions and provide context-aware predictions. The work on Heterogeneous Matrix Factorization (HeteroMF) \cite{jamali2013heteromf} generates context-specific latent vectors of entities using a context-dependent transfer matrix and the original latent vectors of entities. The Tensor Factorization for MAP maximization (TFMAP) model \cite{shi2012tfmap} uses tensor factorization and Mean Average Precision (MAP) objective to model implicit feedback data with contextual information. The newly CARS2 \cite{shi2014cars} and Contextual Operating Tensor (COT) \cite{liu2015cot} models represent the common semantic effects of contexts as contextual operating tensor and represents contexts as latent vectors. And the Hierarchical Interaction Representation (HIR) model \cite{liu2015collaborative} generates the interaction representation via tensor multiplication which can be applied in context-aware recommender system.

\section{Problem Formulation}

\section{Proposed Model}

\section{Parameter Learning}

\section{Experiments}

\section{Conclusions and Future Work}

In this paper, we have proposed two novel multi-behavioral sequential prediction methods, i.e. recurrent log-bilinear model and time-aware recurrent log-bilinear model. In RLBL, we incorporate position-specific transition matrix as well as a recurrent structure. With such an architecture, RLBL can well model both short- and long-term contexts in a historical sequence. Besides, to capture multiple types of behavior in behavioral sequences, behavior-specific matrix is designed and applied for each type of behavior. Then, we further extend the RLBL model and propose a time-aware recurrent log-bilinear model with time-specific transition matrices. Incorporating time difference information, TA-RLBL can further improves the performance of RLBL. The experimental results on four real datasets show that both RLBL and TA-RLBL outperforms the state-of-the-art sequential prediction models.

In the future, we can further investigate the following direction. In CA-RNN, parameters are the same for different users, which does not confirm to practical situations. So, to make better personalized recommendation,+ we need to find a method to determine different parameters for different users or different user groups. Moreover, we didn't take items' features, e.g., categories, descriptions and images of items, into consideration. Thus, incorporating our CA-RNN with features of items may also be our next step.

\small
\bibliographystyle{abbrv}
\bibliography{sigproc}
\vspace{2cm}

\balancecolumns
\end{document}
