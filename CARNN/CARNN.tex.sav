% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}

\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{subfigure}
\usepackage{booktabs}


\begin{document}
\title{Context-aware Sequential Recommendation}

\numberofauthors{4}
\author{
        Anonymous CIKM submission\\
        \\
        Paper ID 0
       }
\maketitle


\begin{abstract}
With the rapid growth of Internet applications, sequential prediction in collaborative filtering has become an emerging and crucial task. Given the behavioral history of a specific user, predicting his or her next choice plays a key role for improvement in various online services. Meanwhile, there are more and more scenarios with multiple types of behaviors, while existing works mainly study sequences with a single type of behavior. As a widely used approach, Markov chain based models are constructed on a strong independence assumption. As two classical neural network based methods for modeling sequences, recurrent neural networks can not well model short-term contexts, and log-bilinear model is not suitable for long-term contexts. In this paper, we propose a Recurrent Log-BiLinear (RLBL) model. It models multiple types of behaviors in historical sequences with behavior-specific transition matrices. Meanwhile, RLBL employs position-specific transition matrices for modeling short-term contexts and uses a recurrent structure for modeling long-term contexts. Moreover, conventional sequential models have problem in handling continuous time difference between input elements in behavioral history, which is a key factor for dynamic prediction. Thus, we further extend RLBL via replacing position-specific transition matrices with time-specific transition matrices, and propose Time-Aware Recurrent Log-BiLinear (TA-RLBL). Experimental results show that the proposed RLBL model and TA-RLBL model yield significant improvements over the state-of-the-art compared methods on three typical datasets, i.e., Movielens-1M dataset, Global Terrorism Database and Tmall dataset with different numbers of behavior types.
\end{abstract}

\keywords{Sequential recommendation, context-awareness, recurrent neural networks} % NOT required for Proceedings

\section{Introduction}
Nowadays, people are overwhelmed by huge amount of information, the exposure to information made people tired of extracting useful and valuable information that they are interested in. This phenomenon turns out to facilitate the development of recommender systems in social networking, e-commerce, online movie and reading websites. Recommender system has now been an important tool for helping people to filter information and locate their preference. Conventional recommendation methods focus on modeling users' preference based on their historical choices of items and always ignore the sequential information. However, user preferences always change with time. Historical behaviors in different time periods have different effects on users' next choice. Accordingly, sequential recommendation is a crucial and emerging task for predicting users' next behaviors.

Nowadays, the importance of sequential information in recommender system has been gradually recognized by researchers in many disciplines, and some efforts have been put into developing CF methods with sequential information \cite{campos2014time}. Markov Chain (MC) based models \cite{yang2010personalizing,rendle2010factorizing,natarajan2013app,chen2015personalized} have been widely used for sequential prediction. MC based models aim to predict the users' next behavior based on the past behaviors in sequential data. A transition matrix is estimated, which can give the probability of an action based on the previous ones. For personalized recommendation, Factorization Personalized Markov Chain (FPMC) \cite{rendle2010factorizing} provide more accurate prediction by factorizing a personalized transition tensor. However, a major problem of MC based models is that all the components are independently combined, indicating that it makes strong independence assumption among multiple factors. Furthermore, MC based methods have been extended via representation learning, which have been applied to next basket recommendation \cite{wang2015learning}. Recently, as another typical representation learning method, Recurrent Neural Networks (RNN) have been employed to model temporal dependency for different applications successfully.  such as sentence modeling tasks \cite{mikolov2010recurrent,mikolov2011extensions,mikolov2011rnnlm} and sequential click prediction \cite{zhang2014sequential}.

\begin{figure}[tb]
\centering
\includegraphics[width=0.5\textwidth]{example.pdf}
\caption{The scenario of app usage prediction as an example of multi-behavioral sequential prediction. This example shows a user's behaviors towards apps in an hour, including downloading, using and uninstalling. At last, we plan to predict what app the user is going to download next.}
\label{fig:Example}
\end{figure}

In this paper, to overcome above shortcomings of conventional methods and model multi-behavioral sequences, we propose two novel sequential prediction methods, i.e., \textbf{Recurrent Log-BiLinear (RLBL)} model and \textbf{Time-Aware Recurrent Log-BiLinear (TA-RLBL)} model. \textit{First}, RLBL uses position-specific transition matrices to capture the short-term contexts in a historical sequence and employs a recurrent architecture for long-term contexts. Our RLBL not only can model the subtle characteristics of the most recent items in a sequence, but also can deal with long-term contexts with a recurrent structure. Meanwhile, to capture the properties of different types of behaviors in historical sequences, we employ behavior-specific transition matrices in RLBL. To the best of our knowledge, our RLBL model is the first work which is designed for predicting multi-behavioral sequences, and well captures the characteristics of both short- and long-term contexts in historical sequences. \textit{Second}, to model time difference information in sequences, TA-RLBL further extends the RLBL model. Rather than specific matrices for each position in RLBL, we use specific matrices, i.e., time-specific transition matrices, for different time difference values between input elements in TA-RLBL. Since it is difficult to estimate matrices for all the continuous time difference values, we divide all the possible temporal values into discrete bins. For a specific time difference value in one time bin, we can calculate the corresponding transition matrix via a linear interpolation of transition matrices of the upper bound and lower bound. Incorporating continuous time difference information, TA-RLBL can further improve the performance of RLBL.

The main contributions of this work are listed as follows:
\begin{itemize}
\item
We firstly address the problem of multi-behavioral sequential prediction, which presents a novel perspective for sequential prediction. And we use behavior-specific matrices to represent the operations of different types of behaviors.

\item
The RLBL model incorporates position-specific matrices and the recurrent structure, which can well model both the short- and long-term contexts in historical sequences.

\item
TA-RLBL uses time-specific matrices to jointly model sequential information and time difference information in one framework, which further improves the performance of RLBL.

\item
Experiments conducted on three real-world datasets show that RLBL and TA-RLBL is effective and clearly outperforms the state-of-the-art methods.

\end{itemize}

The rest of the paper is organized as follows. In section 2, we review some related work on sequential prediction. Then we give the definition of multi-behavioral sequential prediction in section 3. Section 4 and 5 detail our RLBL model and TA-RLBL model respectively. In section 6, we introduce the learning methods of our proposed models. In section 7, we conduct experiments in three real-world datasets and compare with several state-of-the-art methods. Section 8 concludes our work and discusses future research.

\section{Related Works}
In this section, we review several types of methods for sequential prediction and time-aware prediction, i.e., time-aware neighbourhood based methods, sequential pattern based methods, time-aware factorization based methods, markov chain based methods and neural network based methods.

Time-aware neighbourhood models \cite{ding2005time,lathia2009temporal,liu2010online} may be the most natural methods, which employ neighbourhood based algorithms to capture temporal effects via giving more relevance to recent observations and less to past observations. Using frequent pattern mining, sequential pattern based methods \cite{mobasher2002using,hariri2012context} seek sequential patterns that occur most frequently to predict the future. However, both neighbourhood based and sequential pattern based methods are unable to reveal the underlying properties in users' historical sequences. And sequential pattern based methods are time consuming in large scale datasets.

Matrix factorization based methods \cite{mnih2007probabilistic,koren2009matrix,koren2011advances} have become the state-of-the-art approach to collaborative filtering, which have been extended to time-aware factorization based models recently. Tensor Factorization (TF) \cite{xiong2010temporal,bahadori2014fast} treats time intervals as another dimension and generate latent vectors of time intervals via factorization to capture the underlying properties in historical sequences. TimeSVD++ \cite{koren2010collaborative} learns time-aware representations for users and items. However, factorization based models have difficulties in generating latent representations for time intervals which has never or seldom appeared in the training data.

The MC based methods are widely used models for sequential applications \cite{yang2010personalizing}. Via factorization of the probability transition matrix, Factorizing Personalized Markov Chain (FPMC) \cite{rendle2010factorizing} can provide more accurate prediction for each sequence. FPMC is also extended by using user group \cite{natarajan2013app} or incorporating location constraint \cite{cheng2013you}. Recently, some factors of human brain have been added into MC based methods, including interest-forgetting curve \cite{chen2015personalized} and dynamics of boredom \cite{kapoor2015just}. However, the main drawback of MC based models is the linear combination of the past components, which lies in a strong independence assumption and confines the prediction accuracy. MC based methods are then extended by representation learning. Hierarchical Representation Model (HRM) \cite{wang2015learning} learns the representation of behaviors in the last transaction and predicts behaviors for the next transaction. And Personalized Ranking Metric Embedding (PRME) \cite{feng2015personalized} learns embeddings of users according to the location distance. These methods still face a problem that they only model items in the recent history and the items modeled are unordered.

Recently, a few prediction models, especially language models, are proposed based on neural networks. The most classical language model is proposed via a single layer neural network \cite{bengio2003neural}. And language model is recently successfully applied for predicting app usage \cite{baeza2015predicting}. Models based on RNN have been successfully used in modeling sentences \cite{mikolov2010recurrent,mikolov2011extensions,mikolov2011rnnlm}. And RNN also brings satisfying results for sequential click prediction for sponsored search \cite{zhang2014sequential}. However, modeling sequential data, RNN assumes that temporal dependency changes monotonously along with the position in a sequence. This does not confirm to practical situations, especially for the most recent elements of a historical sequence. LBL \cite{mnih2007three} is another classical language model, which models elements of each position in a sequence separately with a specific matrix. And a hierarchical softmax \cite{mnih2009scalable} is utilized to accelerate LBL model. However, when the sequence is too long, LBL can not well model the long-term contexts in a sequence.

\section{Problem Definition}

The multi-behavioral sequential prediction problem we study in this paper can be formulated as follows. We have a set of users and a set of items denoted as $U  = \{ u_1 ,u_2 ,...\}$ and $V  = \{ v_1 ,v_2 ,...\}$ respectively. Multiple types of behaviors are denoted as $B  = \{ b_1 ,b_2 ,...\}$. For each user $u$, behaviors towards items are attached with behavioral types and timestamps. And the sequential behavioral history of user $u$ consists of items $V^u = \{v^u_{1}, v^u_{2}, ...\}$, corresponding behavioral types $B^u = \{b^u_{1}, b^u_{2}, ...\}$ and timestamps $T^u = \{t^u_{1}, t^u_{2}, ...\}$. Given behavioral history of users towards items, the task is to predict what a specific user would choose next under a specific behavior.

Here, taking the applications in e-commerce as an example, there will be four types of behaviors (i.e., clicking, purchasing, adding to favorites and adding to shopping chart) denoted by $\{b_1 ,b_2 ,b_3 ,b_4\}$. The task is to predict which item a user would like to click, purchase, add to favorites or add to shopping chart next. Similarly, in the app usage, there will be three types of behaviors (i.e., downloading, using and uninstalling) denoted by $\{b_1 ,b_2 ,b_3\}$. Then the task becomes predicting which app a user would like to download, use or uninstall next.

\section{Recurrent Log-bilinear Model}

In this section, we detail the recurrent log-bilinear model. We first introduce the RNN model and LBL model. Then we detail the architecture of RLBL with a single type of behaviors and then introduce how RLBL can be employed to model multiple types of behaviors.

\subsection{Recurrent Neural Networks}

The architecture of RNN is shown in Figure \ref{fig:RNN}. It consists of an input layer, an output unit, a hidden layer, as well as inner weight matrices \cite{zhang2014sequential}. The activation values of the hidden layers are computed as:
\begin{equation}
\mathbf{h}^u_{k}  = f\left( {\mathbf{W} \mathbf{h}^u_{k-1} + \mathbf{C} \mathbf{r}_{v^u_k} } \right)~,
\end{equation}
where $\mathbf{h}^u_{k} \in {\mathbb{R}^{d}}$ denotes the hidden representation of user $u$ at position $k$ in a sequence, $\mathbf{r}_{v^u_k} \in {\mathbb{R}^{d}}$ denotes the representation of the $k$th input item of user $u$. $\mathbf{C} \in {\mathbb{R}^{d \times d}}$ and $\mathbf{W} \in {\mathbb{R}^{d \times d}}$ mean the transition matrix for the current items and the previous status respectively. $f(x)$ is the activation function. $\mathbf{W}$ can propagate sequential signals, and $\mathbf{C}$ can capture users' current behavior. This activation process can be repeated iteratively and then the status at each position in a sequence can be calculated.

\subsection{Log-bilinear Model}

The Log-biLinear (LBL) model \cite{mnih2007three} is a deterministic model that may be viewed as a feedforward neural network with a single linear hidden layer \cite{kiros2014multiplicative}. Using LBL for the sequential prediction problem, the final predicted representation of a sequence is generated based on the input items and the transition matrices at each position. As shown in Figure \ref{fig:LBL}, in the LBL model, the representation at next position is a linear prediction:
\begin{equation}
\mathbf{h}^u_{k}  = \sum\limits_{i = 0}^{n - 1} {\mathbf{C}_i \mathbf{r}_{v^u_{k-i}} } ~,
\end{equation}
where $n$ is the max number of elements modeled in a sequence. Similar to the $n$-gram in sentence modeling, and $\mathbf{C}_i \in {\mathbb{R}^{d \times d}}$ denotes the transition matrix for the corresponding position in a sequence.

\begin{figure*}[tb]
\centering
\subfigure[RNN.]{
\begin{minipage}[b]{0.2\textwidth}
\includegraphics[width=1\textwidth]{./RNN.pdf}
\label{fig:RNN}
\end{minipage}
}
\hspace{-4mm}
\subfigure[LBL.]{
\begin{minipage}[b]{0.22\textwidth}
\includegraphics[width=1\textwidth]{./LBL.pdf}
\label{fig:LBL}
\end{minipage}
}
\hspace{-4mm}
\subfigure[RLBL.]{
\begin{minipage}[b]{0.28\textwidth}
\includegraphics[width=1\textwidth]{./RLBL.pdf}
\label{fig:RLBL}
\end{minipage}
}
\hspace{-4mm}
\subfigure[TA-RLBL.]{
\begin{minipage}[b]{0.28\textwidth}
\includegraphics[width=1\textwidth]{./TARLBL.pdf}
\label{fig:TA-RLBL}
\end{minipage}
}
\caption{Overview of different models, i.e., RNN, LBL, RLBL and TA-RLBL.}
\label{models}
\end{figure*}

\subsection{Modeling Single Type of Behaviors}

 As discussed in the previous sections, though both RNN and LBL have achieved satisfying results, they still have their own drawbacks. RNN can not well handle the short-term contexts in a sequence, while LBL can not well model the long-term contexts.

To capture short- and long-term contexts in historical sequences simultaneously, we plan to incorporate the position-specific matrices into the recurrent architecture. As illustrated in Figure \ref{fig:RLBL}, given a user $u$, the hidden representation of the user at the position $k$ in a sequence can be computed as:
\begin{equation} \label{RLBL}
\mathbf{h}^u_{k}  = {\mathbf{W} \mathbf{h}^u_{k-n} + \sum\limits_{i = 0}^{n - 1} {\mathbf{C}_i \mathbf{r}_{v^u_{k-i}} } } ~,
\end{equation}
where $n$ is the max number of input items modeled in one layer of the RLBL model which is called the window width in this paper. The position-specific transition matrices $\mathbf{C}_i \in {\mathbb{R}^{d \times d}}$ captures the impact of the short-term contexts on user behavior. And the characteristics of users' long-term history are modeled via the recurrent framework. It obvious that, when we only consider one input item in each layer and set the window width $n=1$, the formulation of RLBL will be as the same as that of RNN ignoring the nonlinear activation function.

Notice that, when the sequence is shorter than the window width or the predicted position is at the very first part of a sequence, we have $k<n$. Then, Equation \ref{RLBL} should be rewritten as:
\begin{equation}
\mathbf{h}^u_{k}  = {\mathbf{W} \mathbf{h}^u_{0} + \sum\limits_{i = 0}^{k - 1} {\mathbf{C}_i \mathbf{r}_{v^u_{k-i}} } } ~,
\end{equation}
where $\mathbf{h}^u_{0}  = \mathbf{u}_0$, denoting the initial status of users. The initial status of all users should be the same because there does not exist any personal information when a user has not selected an item. This representation $\mathbf{u}_0$ can be used to model cold start users. This equation can be viewed as the same as that of a regular LBL model.

\subsection{Modeling Multiple Types of Behaviors}

\noindent Although there exist situations with one type of behavior, e.g., buying in e-commerce and clicking on websites, there are much more applications with multiple types of behaviors towards items. For instance, users will click items, purchase items and add items to favorites in e-commerce. And users may download apps, use apps and uninstall apps. Thus, it is necessary to model multi-behavioral sequences and collaboratively predict what a user will choose next under a specific behavior.

We can simply treat different behaviors towards one item as different item representations in conventional models. However, it is hard to model the correlation among different behaviors towards one item. Here, we incorporate behavior-specific matrices to capture properties of multiple types of behaviors. Then, the representation at position $k$ can be calculated as:
\begin{equation}
\mathbf{h}^u_{k}  = {\mathbf{W} \mathbf{h}^u_{k-n} + \sum\limits_{i = 0}^{n - 1} {\mathbf{C}_i \mathbf{M}_{b^u_{k-i}} \mathbf{r}_{v^u_{k-i}} } } ~,
\end{equation}
where $\mathbf{M}_{b^u_{i}} \in {\mathbb{R}^{d \times d}}$ denotes a behavior-specific transition matrix modeling the corresponding behavior on the $i$th item of user $u$. Note that, behavior-specific matrices can be omitted if there is only one type of behavior. Incorporating behavior-specific matrices, RLBL is the first approach to model the underlying properties of different types of behaviors in historical sequences.

Now, as in matrix factorization, via calculating inner product, the prediction of whether user $u$ would conduct behavior $b$ on item $v$ next can be made as:
\begin{equation} \label{prediction}
y_{u,k+1,b,v}  = (\mathbf{s}^u_k)^T \mathbf{M}_b \mathbf{r}_v  = (\mathbf{h}^u_{k} + \mathbf{u}_u)^T \mathbf{M}_b \mathbf{r}_v  ~,
\end{equation}
where $\mathbf{u}_u \in {\mathbb{R}^{d}}$ is a latent representation for user $u$, and $\mathbf{s}^u_k$ denotes the representation for the status of user $u$ at the sequential position $k$, containing dynamic representation $\mathbf{h}^u_{k}$ and static representation $\mathbf{u}_u$.

\section{Time-aware RLBL Model}

Sequential models often ignore the continuous time difference between input elements. The time difference information is important for prediction considering that shorter time difference has more significant impact to the future comparing with longer time difference. For instance, suppose a user buys item $v_a$ last night and item $v_b$ last month. It is more probably that the user's choice about what to buy next is mainly influenced by item $v_a$ because of the similar interest in a short period. In contrast, if item $v_b$ is bought last mourning, it is probably that both item $v_a$ and $v_b$ have similar impact to the user's choice because of the similar interest in one day. Moreover, as the purchasing behavior of some items is periodical such as buying tooth paste every month, the effect of time difference becomes more significant in such situations.

Accordingly, in this section, we extend our RLBL model with time difference information and introduce the time-aware recurrent log-bilinear model.

\subsection{Proposed Model}

As discussed above, it will be reasonable if we incorporate time difference information in our RLBL model. Thus, we replace position-specific transition matrices in RLBL with time-specific transition matrices and propose a time-aware RLBL model. As shown in Figure \ref{fig:TA-RLBL}, given a user $u$, his or her representation at position $k$ can be calculated as:
\begin{equation} \label{TA-RLBL}
\mathbf{h}^u_{k}  = {\mathbf{W} \mathbf{h}^u_{k-n} + \sum\limits_{i = 0}^{n - 1} {\mathbf{T}_{t^u_{k}-t^u_{k-i}} \mathbf{r}_{v^u_{k-i}} } } ~,
\end{equation}
where $\mathbf{T}_{t^u_{k}-t^u_{k-i}} \in {\mathbb{R}^{d \times d}}$ denotes the time-specific transition matrix for the time difference ${t^u_{k}-t^u_{k-i}}$ between the timestamp $t^u_{k-i}$ of each item in each layer of TA-RLBL and the current timestamp $t^u_{k}$. The time-specific transition can capture the time-aware impacts in the most recent history.

Moreover, as in RLBL, when $k<n$, Equation \ref{TA-RLBL} should be rewritten as:
\begin{equation} \label{TA-RLBL-0}
\mathbf{h}^u_{k}  = {\mathbf{W} \mathbf{h}^u_{0} + \sum\limits_{i = 0}^{k - 1} {\mathbf{T}_{t^u_{k}-t^u_{k-i}} \mathbf{r}_{v^u_{k-i}} } } ~,
\end{equation}
where $\mathbf{h}^u_{0}  = \mathbf{u}_0$, denoting the initial status of users. And to model multiple types of behavior, behavior-specific transition matrices are also applied in TA-RLBL model:
\begin{equation} \label{M-TA-RLBL}
\mathbf{h}^u_{k}  = {\mathbf{W} \mathbf{h}^u_{k-n} + \sum\limits_{i = 0}^{n - 1} {\mathbf{T}_{t^u_{k}-t^u_{k-i}} \mathbf{M}_{b^u_{k-i}} \mathbf{r}_{v^u_{k-i}} } } ~.
\end{equation}
Then, similar to RLBL, the prediction of whether user $u$ would conduct behavior $b$ on item $v$ next can be computed by using Equation \ref{prediction}.

\subsection{Linear Interpolation for Learning Transition Matrices}

If we learn a distinct matrix for each possible continuous time difference value, we need to estimate a great number of time-specific transition matrices. Here, we partition the range of all the possible time difference values into discrete bins equally. Only the transition matrices of the upper and lower bounds of time bins are needed to be learned in our model. For the time difference values in a time bin, their transition matrices can be calculated via a linear interpolation. Mathematically, the time-specific transition matrix $T_{t_d}$ for time difference value $t_d$ can be calculated as:
\begin{equation}
\mathbf{T}_{t_d}  = \frac{{\left[ {\mathbf{T}_{L(t_d)} (U(t_d) - t_d) + \mathbf{T}_{U(t_d)} (t_d - L(t_d))} \right]}}{{\left[ {(U(t_d) - t_d) + (t_d - L(t_d))} \right]}} ~,
\end{equation}
where $U(t_d)$ and $L(t_d)$ denote the upper bound and lower bound of time difference $t_d$. Such a linear interpolation method can solve the problem of learning time-specific transition matrices for continuous time difference.

We have detailed the RLBL and TA-RLBL model. Both models can well capture sequential information. If there exists explicit time information, TA-RLBL model can achieve better performance than that of RLBL model. And if the dataset is not associated with detailed time information, RLBL mode will be more suitable than TA-RLBL model which is established based on the time difference between input elements. Both models are constructed under the same framework and can be applied according to actual situations.

\section{Parameter Learning}

In this section, we introduce the learning process of both the RLBL model and the TA-RLBL model with skip-gram \cite{mikolov2013distributed} and back propagation through time \cite{rumelhart1988learning}.

\begin{table*}[tb]
  \centering\scriptsize
  \caption{Experimental summarization.}
    \begin{tabular}{ccccc}
    \toprule
    dataset & scenario & \#behavioral types & behaviors & behavior to predict \\
    \midrule
    Movielens & watching movies & 5     & rating 5, 4 , 3, 2, 1 stars & rating 5 or 4 stars \\
    GTD   & terrorist attack & 7     & armed, unarmed, assassination, bombing, facility, hijacking, hostage & attack (all types) \\
    Tmall & e-commerce & 4     & clicking, purchasing, adding to favorites, adding to shopping cart & purchasing \\
    \bottomrule
    \end{tabular}%
  \label{tab:summarization}%
\end{table*}%

\subsection{Learning of RLBL}

The training objective of the Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document \cite{mikolov2013distributed}. Skip-gram model is widely used in word embedding, as well as in app usage prediction \cite{baeza2015predicting}. Formally, using skip-gram framework for learning RLBL, the objective is to maximize:
\begin{equation} \label{Learning of RLBL}
\sum\limits_{u \in U} {\sum\limits_{k = 0}^{l(u)-1} {\frac{{\exp \left( {(\mathbf{s}^u_k)^T \mathbf{M}_b \mathbf{r}_v } \right)}}{{\sum\limits_{\{ b',v'\}  \in N (u,k)} {\exp \left( {(\mathbf{s}^u_k)^T \mathbf{M}_b' \mathbf{r}_v' } \right)} }}} } ~,
\end{equation}
where $U$ is the set of users, $l(u)$ denotes the length of the behavioral sequence of user $u$, $N (u,k)$ denotes a set of negative samples of user $u$ at position $k$, $\{b,v\}$ is a positive sample and $\{b',v'\}$ is a negative sample.

The maximization of Equation \ref{Learning of RLBL} can be obtained via minimizing the negative log likelihood, and then the objective function is obtained:
\begin{equation} \label{Objective}
J  = \sum\limits_{u \in U} {\sum\limits_{k = 0}^{l(u)-1} {L (u,k)} }  + \frac{\lambda}{2} \left\| {\mathbf{\Theta} } \right\|^2 ~,
\end{equation}
where
\begin{displaymath}
L (u,k) = \ln \left( {\sum\limits_{\{ b',v'\}  \in N (u,k)} {\exp \left( {(\mathbf{s}^u_k)^T \mathbf{M}_b' \mathbf{r}_v' } \right)} } \right) - {(\mathbf{s}^u_k)^T \mathbf{M}_b \mathbf{r}_v } ~,
\end{displaymath}
and $\mathbf{\Theta}  = \left\{ {\mathbf{U},\mathbf{R},\mathbf{W},\mathbf{C},\mathbf{M}} \right\}$ denotes all the parameters to be estimated, $\lambda $ is a parameter to control the power of regularization. Thus, the derivations of $J$ with respect to the parameters can be calculated as:
\begin{displaymath}
\frac{{\partial {J}}}{{\partial {\mathbf{r}_{v}}}} =  - \sum\limits {(\mathbf{M}_b)^T{\mathbf{s}^u_k}}  + \lambda {\mathbf{r}_{v}} ~,
\end{displaymath}
\begin{displaymath}
\frac{{\partial {J}}}{{\partial {\mathbf{M}_{b}}}} =  - \sum\limits {{\mathbf{s}^u_k}({\mathbf{r}_{v}})^T}  + \lambda {\mathbf{M}_{b}} ~,
\end{displaymath}
\begin{displaymath}
\frac{{\partial {J}}}{{\partial {\mathbf{r}_{v'}}}} = \sum\limits (\mathbf{M}_{b'})^T{\frac{{{s_{u,k}}\exp \left( {(\mathbf{s}^u_k)^T \mathbf{M}_b' \mathbf{r}_v' } \right)}}{{\sum\limits_{\{b'',v''\} \ne \{b',v'\} } {\exp \left( {(\mathbf{s}^u_k)^T \mathbf{M}_b'' \mathbf{r}_v''p } \right)} }}}  + \lambda {\mathbf{r}_{v'}} ~,
\end{displaymath}
\begin{displaymath}
\frac{{\partial {J}}}{{\partial {\mathbf{M}_{b'}}}} = \sum\limits {\frac{{{s_{u,k}}\exp \left( {(\mathbf{s}^u_k)^T \mathbf{M}_b' \mathbf{r}_v' } \right)}}{{\sum\limits_{\{b'',v''\} \ne \{b',v'\} } {\exp \left( {(\mathbf{s}^u_k)^T \mathbf{M}_b'' \mathbf{r}_v''p } \right)} }}}{r_{v'}}^T  + \lambda {\mathbf{M}_{b'}} ~,
\end{displaymath}
\begin{displaymath}
\frac{{\partial {J}}}{{\partial {\mathbf{u}_{u}}}} = \sum {\left( {\frac{{\sum {{\mathbf{M}_b' \mathbf{r}_v'}\exp \left( {(\mathbf{s}^u_k)^T \mathbf{M}_b' \mathbf{r}_v' } \right)} }}{{\sum {\exp \left( {(\mathbf{s}^u_k)^T \mathbf{M}_b' \mathbf{r}_v' } \right)} }} - {\mathbf{M}_b \mathbf{r}_v}} \right)}  + \lambda {\mathbf{u}_u} ~,
\end{displaymath}
\begin{displaymath}
\frac{{\partial {J}}}{{\partial {\mathbf{h}^u_{k}}}} = \sum {\left( {\frac{{\sum {{\mathbf{M}_b' \mathbf{r}_v'}\exp \left( {(\mathbf{s}^u_k)^T \mathbf{M}_b' \mathbf{r}_v' } \right)} }}{{\sum {\exp \left( {(\mathbf{s}^u_k)^T \mathbf{M}_b' \mathbf{r}_v' } \right)} }} - {\mathbf{M}_b \mathbf{r}_v}} \right)} ~.
\end{displaymath}

The derivations of the output layer have been calculated. Under each layer of the recurrent structure, similar to the conventional RNN model, RLBL can be trained by using the Back Propagation Through Time (BPTT) algorithm \cite{rumelhart1988learning}, which has been used in practical sequential prediction models \cite{zhang2014sequential}.

For user $u$, given the derivation $\frac{{\partial {J}}}{{\partial {\mathbf{h}^u_{k}}}}$ of the representation $\mathbf{h}^u_{k}$ at sequential position $k$, the corresponding gradient of parameters at that hidden layer can be calculated as:
\begin{displaymath}
\frac{{\partial J }}{{\partial \mathbf{h}^u_{k-n} }} = \mathbf{W}^T  \frac{{\partial J }}{{\partial \mathbf{h}^u_{k}}} ~,
\end{displaymath}
\begin{displaymath}
\frac{{\partial J }}{{\partial \mathbf{W}^T}} = \frac{{\partial J }}{{\partial \mathbf{h}^u_{k} }} (\mathbf{h}^u_{k-n})^T ~,
\end{displaymath}
\begin{displaymath}
\frac{{\partial J }}{{\partial \mathbf{r}_{v^u_{k-i}} }} = (\mathbf{M}_{b^u_{k-i}})^T (\mathbf{C}_i)^T \frac{{\partial J }}{{\partial \mathbf{h}^u_{k} }} ~,
\end{displaymath}
\begin{displaymath}
\frac{{\partial J }}{{\partial \mathbf{C}_i }} = \frac{{\partial J }}{{\partial \mathbf{h}^u_{k} }} (\mathbf{r}_{v^u_{k-i}})^T (\mathbf{M}_{b^u_{k-i}})^T ~,
\end{displaymath}
\begin{displaymath}
\frac{{\partial J }}{{\partial \mathbf{M}_{b^u_{k-i}} }} = (\mathbf{C}_i)^T \frac{{\partial J }}{{\partial \mathbf{h}^u_{k} }} (\mathbf{r}_{v^u_{k-i}})^T ~.
\end{displaymath}
This process can be repeated iteratively, and the gradients of all the parameters are obtained. Then, the model can be learned via Stochastic Gradient Descent (SGD) until converge.

\subsection{Learning of TA-RLBL}

For learning of TA-RLBL, skip-gram \cite{mikolov2013distributed} can also be applied. We also need to maximize Equation \ref{Learning of RLBL}, and then the objective function is the same as Equation \ref{Objective}, except $\mathbf{\Theta}  = \left\{ {\mathbf{U},\mathbf{R},\mathbf{W},\mathbf{T},\mathbf{M}} \right\}$ in TA-RLBL. Thus, in TA-RLBL, the derivations of $J$ with respect to the parameters in Equation \ref{Objective} can be computed as same as in RLBL.

Then, as in RLBL, applying BPTT, the corresponding gradient of parameters at each hidden layer can be calculated as:
\begin{displaymath}
\frac{{\partial J }}{{\partial \mathbf{h}^u_{k-n} }} = \mathbf{W}^T  \frac{{\partial J }}{{\partial \mathbf{h}^u_{k}}} ~,
\end{displaymath}
\begin{displaymath}
\frac{{\partial J }}{{\partial \mathbf{W}^T}} = \frac{{\partial J }}{{\partial \mathbf{h}^u_{k} }} (\mathbf{h}^u_{k-n})^T ~,
\end{displaymath}
\begin{displaymath}
\frac{{\partial J }}{{\partial \mathbf{r}_{v^u_{k-i}} }} = (\mathbf{M}_{b^u_{k-i}})^T (\mathbf{C}_i)^T \frac{{\partial J }}{{\partial \mathbf{h}^u_{k} }} ~,
\end{displaymath}
\begin{displaymath}
\frac{{\partial J }}{{\partial \mathbf{C}_i }} = \frac{{\partial J }}{{\partial \mathbf{h}^u_{k} }} (\mathbf{r}_{v^u_{k-i}})^T (\mathbf{M}_{b^u_{k-i}})^T ~,
\end{displaymath}
\begin{displaymath}
\frac{{\partial J }}{{\partial \mathbf{M}_{b^u_{k-i}} }} = (\mathbf{C}_i)^T \frac{{\partial J }}{{\partial \mathbf{h}^u_{k} }} (\mathbf{r}_{v^u_{k-i}})^T ~.
\end{displaymath}
The process above can be repeated iteratively, and we can obtain all the gradients. After that, the model can be trained via SGD until converge.

\section{Experiments}

In this section, we empirically investigate the performance of RLBL and TA-RLBL. As shown in Table \ref{tab:summarization}, we conduct our experiments in three scenarios with different numbers of behavioral types. We first introduce our experimental settings. Then we conduct experiments on comparing RLBL and TA-RLBL with different window width and experiments on comparing single behavior and multiple behaviors. We also give comparison of our models and some state-of-the-art methods with varying dimensionality. Finally, we study the performance of models under different length of behavioral history and the convergence properties of models.

\subsection{Experimental Settings}
Our experiments are conducted on three real datasets with different numbers of behavioral types:
\begin{itemize}
\item \textbf{Movielens-1M}\footnote{http://grouplens.org/datasets/movielens/} is a widely used dataset, associated with timestamps, for the rating prediction in recommender systems. The ratings are divided into five levels, indicating users' different levels of preference, which can be viewed as five different types of behaviors. With this dataset, we plan to predict which movie a user will rate 5 or 4 stars next.

\item \textbf{Global Terrorism Database (GTD)}\footnote{http://www.start.umd.edu/gtd/} includes more than 125,000 terrorist incidents that have occurred all around the world since 1970. This dataset consists 7 behavioral types, i.e., different attracting types, indicated in Table \ref{tab:summarization}. For social good, we would like to predict which province or state a terrorist organization will attack. Thus, it is available for us to take action before accidents happen and save people's life.

\item \textbf{Tmall}\footnote{https://102.alibaba.com/competition/addDiscovery/index.htm} is a dataset from Tmall\footnote{https://www.tmall.com/}, one of the biggest online shopping websites in China. The temporal information in the dataset is extracted based on day level. It contains four different types of behaviors: clicking, purchasing, adding to favorites and adding to shopping cart. It suits for the task of collaborative prediction on multi-behavioral sequences. On this dataset, we plan to predict what users will purchase next.
\end{itemize}

For each behavioral sequence of these three datasets, we use first $70\%$ of the items in the sequence for training, following $10\%$ data as the validation set for tuning parameters, i.e., the dimensionality of latent representations, and remaining $20\%$ for testing. In our methods, the regularization parameter is set as $\lambda = 0.01$. And we use line search to select learning rates in each iteration.

\begin{figure}[!tb]
\centering
\subfigure[Performance on the Movielens dataset.]{
\begin{minipage}[b]{0.5\textwidth}
\includegraphics[width=1\textwidth]{./result3.eps}
\label{dimen3}
\end{minipage}
}
\subfigure[Performance on GTD.]{
\begin{minipage}[b]{0.5\textwidth}
\includegraphics[width=1\textwidth]{./result2.eps}
\label{dimen2}
\end{minipage}
}
\subfigure[Performance on the Tmall dataset.]{
\begin{minipage}[b]{0.5\textwidth}
\includegraphics[width=1\textwidth]{./result1.eps}
\label{dimen1}
\end{minipage}
}
\caption{Performance comparison with varying dimensionality $d$.}
\label{fig:comparision}
\end{figure}

We compare RLBL and TA-RLBL with both conventional and state-of-the-art sequential methods:
\begin{itemize}
\item \textbf{POP} is a naive baseline method that recommends the most popular items to the users.
\item \textbf{MF} \cite{mnih2007probabilistic} is one of the state-of-the-art methods for conventional collaborative filtering.
\item \textbf{MC} is a classical sequential model and is used as a sequential baseline method.
\item \textbf{TF} \cite{xiong2010temporal} extends MF to three dimensions where the temporal information is modeled as the additional dimension.
\item \textbf{FPMC} \cite{rendle2010factorizing} is a widely-used method for sequential recommendation based on markov chain.
\item \textbf{HRM} \cite{wang2015learning} learns the representation of behaviors in the previous transaction and predicts next behaviors.
\item \textbf{RNN} \cite{zhang2014sequential} is a state-of-the-art method for the sequential prediction, which has been successfully applied in sentence modeling and click prediction.
\end{itemize}

As these methods can not model multi-behavioral sequences, when we conduct compared methods on multi-behavioral datasets, we treat different behaviors towards one item as different elements in historical sequences.

Moreover, to investigate the performance of all these methods, we select several widely-used evaluation metrics for our experiments:

\begin{itemize}
\item \textbf{Recall@k} and \textbf{F1-score@k} are two important metrics for ranking tasks. The evaluation score for our experiments is computed according to where the next selected item appears in the predicted list. We report recall@k and F1-score@k with $k=1$, $2$, $5$ and $10$ in our experiments. The larger the value, the better the performance.

\item \textbf{Mean Average Precision (MAP)} is another widely used global evaluation in ranking tasks, which measure the quality of the whole ranking list. Top-bias property of MAP is particularly significant in evaluating ranking tasks such as top-n recommendation. The larger the value, the better the performance.
\end{itemize}

\begin{table*}[!tb]
\caption{Comparison of RLBL and TA-RLBL with varying window width $n$.}
\centering\scriptsize
\subtable[Performance on the Movielens dataset.]{
    \begin{tabular}{ccccccccccc}
    \toprule
    method & n     & recall@1 & recall@2 & recall@5 & recall@10 & F1-score@1 & F1-score@2 & F1-score@5 & F1-score@10 & MAP \\
    \midrule
    \multirow{7}[0]{*}{RLBL} & 2     & 0.0067  & 0.0103  & 0.0333  & 0.0508  & 0.0067  & 0.0069  & 0.0111  & 0.0093  & 0.0377  \\
          & 3     & 0.0070  & 0.0104  & 0.0334  & 0.0510  & 0.0070  & 0.0070  & 0.0111  & 0.0093  & 0.0381  \\
          & 4     & 0.0070  & 0.0107  & 0.0338  & 0.0520  & 0.0070  & 0.0072  & 0.0113  & 0.0095  & 0.0385  \\
          & 5     & 0.0070  & 0.0108  & 0.0343  & 0.0527  & 0.0070  & 0.0072  & 0.0114  & 0.0096  & 0.0386  \\
          & 6     & 0.0071  & 0.0112  & 0.0354  & 0.0538  & 0.0071  & 0.0074  & 0.0118  & 0.0098  & 0.0395  \\
          & 7     & 0.0070  & 0.0111  & 0.0354  & 0.0543  & 0.0070  & 0.0074  & 0.0118  & 0.0099  & 0.0393  \\
          & 8     & 0.0070  & 0.0108  & 0.0351  & 0.0535  & 0.0070  & 0.0072  & 0.0117  & 0.0097  & 0.0390  \\
    \midrule
    \multirow{7}[0]{*}{TA-RLBL} & 2     & 0.0070  & 0.0106  & 0.0343  & 0.0529  & 0.0070  & 0.0071  & 0.0114  & 0.0096  & 0.0388  \\
          & 3     & 0.0071  & 0.0105  & 0.0338  & 0.0523  & 0.0071  & 0.0071  & 0.0113  & 0.0094  & 0.0385  \\
          & 4     & 0.0071  & 0.0108  & 0.0337  & 0.0522  & 0.0071  & 0.0073  & 0.0113  & 0.0095  & 0.0388  \\
          & 5     & 0.0070  & 0.0110  & 0.0366  & 0.0553  & 0.0068  & 0.0074  & 0.0123  & 0.0101  & 0.0396  \\
          & 6     & \textbf{0.0072} & \textbf{0.0115} & \textbf{0.0372} & \textbf{0.0554} & \textbf{0.0072} & \textbf{0.0076} & \textbf{0.0124} & \textbf{0.0101} & \textbf{0.0404} \\
          & 7     & 0.0070  & 0.0115  & 0.0362  & 0.0549  & 0.0070  & 0.0076  & 0.0121  & 0.0100  & 0.0398  \\
          & 8     & 0.0070  & 0.0110  & 0.0348  & 0.0539  & 0.0070  & 0.0073  & 0.0118  & 0.0100  & 0.0392  \\
    \bottomrule
    \end{tabular}%
  \label{tab:mine_movielens}%
}
\qquad
\subtable[Performance on GTD.]{
    \begin{tabular}{ccccccccccc}
    \toprule
    method & $n$     & recall@1 & recall@2 & recall@5 & recall@10 & F1-score@1 & F1-score@2 & F1-score@5 & F1-score@10 & MAP \\
    \midrule
    \multirow{7}[0]{*}{RLBL} & 2     & 0.1577  & 0.2448  & 0.4378  & 0.6104  & 0.1577  & 0.1632  & 0.1459  & 0.1110  & 0.2930  \\
          & 4     & 0.1642  & 0.2691  & 0.4676  & 0.6395  & 0.1642  & 0.1794  & 0.1559  & 0.1163  & 0.3082  \\
          & 6     & 0.1624  & 0.2686  & 0.4768  & 0.6468  & 0.1624  & 0.1791  & 0.1589  & 0.1176  & 0.3090  \\
          & 9     & 0.1580  & 0.2848  & 0.4865  & \textbf{0.6748} & 0.1580  & 0.1899  & 0.1622  & \textbf{0.1227} & 0.3153  \\
          & 10    & 0.1569  & 0.2806  & 0.4846  & 0.6659  & 0.1569  & 0.1871  & 0.1615  & 0.1211  & 0.3130  \\
          & 15    & 0.1567  & 0.2660  & 0.4682  & 0.6470  & 0.1567  & 0.1773  & 0.1561  & 0.1176  & 0.3053  \\
          & 20    & 0.1690  & 0.2775  & 0.4872  & 0.6572  & 0.1690  & 0.1850  & 0.1624  & 0.1195  & 0.3165  \\
    \midrule
    \multirow{7}[0]{*}{TA-RLBL} & 2     & 0.1642  & 0.2763  & 0.4740  & 0.6451  & 0.1697  & 0.1842  & 0.1580  & 0.1173  & 0.3117  \\
          & 4     & 0.1681  & 0.2758  & 0.4719  & 0.6411  & 0.1686  & 0.1839  & 0.1573  & 0.1166  & 0.3187  \\
          & 6     & 0.1678  & 0.2758  & 0.4833  & 0.6524  & 0.1678  & 0.1839  & 0.1611  & 0.1186  & 0.3146  \\
          & 9     & 0.1634  & \textbf{0.2895} & \textbf{0.4926} & 0.6730  & 0.1634  & \textbf{0.1930} & \textbf{0.1642} & 0.1224  & \textbf{0.3199} \\
          & 10    & 0.1622  & 0.2864  & 0.4910  & 0.6672  & 0.1622  & 0.1909  & 0.1637  & 0.1213  & 0.3180  \\
          & 15    & 0.1618  & 0.2731  & 0.4746  & 0.6527  & 0.1618  & 0.1821  & 0.1582  & 0.1187  & 0.3107  \\
          & 20    & \textbf{0.1697} & 0.2849  & 0.4839  & 0.6629  & \textbf{0.1697} & 0.1899  & 0.1613  & 0.1205  & 0.3197  \\
    \bottomrule
    \end{tabular}%
  \label{tab:mine_GTD}%
}
\qquad
\subtable[Performance on the Tmall dataset.]{
    \begin{tabular}{ccccccccccc}
    \toprule
    method & n     & recall@1 & recall@2 & recall@5 & recall@10 & F1-score@1 & F1-score@2 & F1-score@5 & F1-score@10 & MAP \\
    \midrule
    \multirow{7}[0]{*}{RLBL} & 2     & 0.1507  & 0.2170  & 0.3712  & 0.4690  & 0.1507  & 0.1447  & 0.1237  & 0.0853  & 0.2704  \\
          & 3     & 0.1480  & 0.2515  & \textbf{0.4118} & 0.5176  & 0.1480  & 0.1677  & \textbf{0.1373} & 0.0941  & 0.2781  \\
          & 4     & 0.1467  & 0.2311  & 0.3646  & 0.4953  & 0.1467  & 0.1541  & 0.1215  & 0.0901  & 0.2689  \\
          & 5     & \textbf{0.1600} & 0.2158  & 0.3975  & 0.5519  & \textbf{0.1600} & 0.1439  & 0.1325  & 0.1003  & \textbf{0.2836} \\
          & 6     & 0.1502  & 0.2272  & 0.3822  & \textbf{0.5596} & 0.1502  & 0.1515  & 0.1274  & \textbf{0.1017} & 0.2806  \\
          & 7     & 0.1493  & \textbf{0.2553} & 0.4074  & 0.5272  & 0.1493  & \textbf{0.1702} & 0.1358  & 0.0959  & 0.2819  \\
          & 8     & 0.1387  & 0.2324  & 0.4019  & 0.5395  & 0.1387  & 0.1549  & 0.1340  & 0.0981  & 0.2770  \\
    \midrule
    \multirow{7}[0]{*}{TA-RLBL} & 2     & 0.1351  & 0.2302  & 0.3669  & 0.4493  & 0.1351  & 0.1535  & 0.1223  & 0.0817  & 0.2608  \\
          & 3     & 0.1268  & 0.1931  & 0.3497  & 0.4541  & 0.1268  & 0.1287  & 0.1166  & 0.0826  & 0.2579  \\
          & 4     & 0.1441  & 0.2450  & 0.4084  & 0.4780  & 0.1441  & 0.1633  & 0.1361  & 0.0869  & 0.2820  \\
          & 5     & 0.1413  & 0.2366  & 0.3871  & 0.5461  & 0.1413  & 0.1577  & 0.1290  & 0.0993  & 0.2804  \\
          & 6     & 0.1253  & 0.2039  & 0.4081  & 0.4454  & 0.1253  & 0.1359  & 0.1360  & 0.0810  & 0.2521  \\
          & 7     & 0.1234  & 0.2213  & 0.3556  & 0.4412  & 0.1234  & 0.1475  & 0.1185  & 0.0802  & 0.2523  \\
          & 8     & 0.1198  & 0.2063  & 0.3472  & 0.4247  & 0.1198  & 0.1375  & 0.1157  & 0.0772  & 0.2454  \\
    \bottomrule
    \end{tabular}%
  \label{tab:mine_tmall}%
}
\label{tab:mine}%
\end{table*}

\subsection{Performance Comparison with Compared Methods}

We compare RLBL, TA-RLBL and compared methods with varying dimensionality $d$ on the three datasets. In Figure \ref{fig:comparision}, compared to the baseline performance of POP, MF, MC and TF have very similar performance improvement on the three datasets. They all have their shortcomings. MF can not model sequential information, MC can not model collaborative information, and TF has difficulty in predicting future behaviors. Jointly modeling sequential information and collaborative information, FPMC achieves great improvement comparing with the three methods. Learning latent representations of recent behaviors, HRM further improves the performance of FPMC. And RNN is clearly the best one among the compared methods. Moreover, we can observe that, our proposed RLBL model and TA-RLBL model achieve the best performance on all the three datasets in terms of all the metrics. Using the best scores of each method, comparing with RNN, the MAP improvements of RLBL are $9.18\%$, $21.27\%$ and $\%16.64$ on Movielens, GTD and Tmall respectively, and the MAP improvements of TA-RLBL are $11.62\%$, $23.04\%$ and $15.31\%$ on the three datasets respectively. These results show the superiority of our methods brought by multi-behavior modeling and incorporating position-specific in RLBL or time specific transition in TA-RLBL.

In Figure \ref{fig:comparision}, We can also observe the performance curves of all the methods along with dimensionality $n$. All the curves clearly show the great advantages of RLBL and TA-RLBL comparing with other compared methods with different dimensionality. The curves also show that the performances of our models are stable in a large range on different datasets evaluated by different metrics. And even not with the best dimensionality, our methods can still outperform compared methods. According to the curves, for the rest of our experiments, we select the dimensionality as $d=8$ and report corresponding performances.

\subsection{RLBL VS. TA-RLBL}

Table \ref{tab:mine} illustrates the performance comparison between our RLBL model and TA-RLBL model under varying window size $n$. In most cases, we can clearly observe that, TA-RLBL performs better that RLBL. On the Movielens dataset, the performances of the two models are stable and TA-RLBL clearly achieves a better performance evaluated by all the metrics under all the window width. On GTD, TA-RLBL performs better than RLBL mostly, especially evaluated by the global metrics MAP. But under window width $n=9$, the RLBL model achieves a little better recall@10 and F1-score@10 scores. These observations may indicate that replacing position-specific transition with time-specific transition can achieve better performance when there exists explicit time information. However, on the Tmall dataset, RLBL performs better than TA-RLBL in most cases. The reasons may be that the Tmall dataset only has sequential information in each day, but has no more detailed time information. Therefore, behaviors in one day are modeled equally and time-specific transition in TA-RLBL brings performance reduction. Accordingly, it is necessary to select a proper model between RLBL and TA-RLBL according to whether there is enough detailed time information in the dataset.

Table \ref{tab:mine} can also helps us to select the best window width $n$ for the rest of our experiments. Performances of our models on Movielens are stable and the best parameter is obviously achieved at $n=6$. On Tmall and GTD, the performances are not that stable evaluated by different metrics. So we select the best parameters according to the global metric MAP. Then the best window width for GTD is $n=9$, and the best window width for the Tmall datast is $n=5$. For the rest of our experiments, we report the performance of RLBL and TA-RLBL under the best window width we have achieved above.

\begin{table}[tb]
\caption{Comparison of multiple behaviors and single behavior.}
\centering\scriptsize
\subtable[Performance on the Movielens dataset.]{
    \begin{tabular}{cccccc}
    \toprule
    behaviors & method & recall@1 & recall@5 & recall@10 & MAP \\
    \midrule
    \multirow{3}[0]{*}{single} & RNN   & 0.0063  & 0.0318  & 0.0484  & 0.0362  \\
          & RLBL  & 0.0068  & 0.0343  & 0.0519  & 0.0384  \\
          & TA-RLBL & 0.0068  & 0.0360  & 0.0535  & 0.0392  \\
    \midrule
    \multirow{2}[0]{*}{multiple} & RLBL  & 0.0071  & 0.0354  & 0.0538  & 0.0395  \\
          & TA-RLBL & \textbf{0.0072} & \textbf{0.0372} & \textbf{0.0554} & \textbf{0.0404} \\
    \bottomrule
    \end{tabular}%
  \label{tab:single_movielens}%
}
\qquad
\subtable[Performance on GTD.]{
    \begin{tabular}{cccccc}
    \toprule
    behaviors & method & recall@1 & recall@5 & recall@10 & MAP \\
    \midrule
    \multirow{3}[0]{*}{single} & RNN   & 0.1216  & 0.4168  & 0.5912  & 0.2600  \\
          & RLBL  & 0.1254  & 0.4723  & 0.6665  & 0.2853  \\
          & TA-RLBL & 0.1298  & 0.4783  & 0.6648  & 0.2896  \\
    \midrule
    \multirow{2}[0]{*}{multiple} & RLBL  & 0.1580  & 0.4865  & \textbf{0.6748} & 0.3153  \\
          & TA-RLBL & \textbf{0.1634} & \textbf{0.4926} & 0.6730  & \textbf{0.3199} \\
    \bottomrule
    \end{tabular}%
  \label{tab:single_GTD}%
}
\qquad
\subtable[Performance on the Tmall dataset.]{
    \begin{tabular}{cccccc}
    \toprule
    behaviors & method & recall@1 & recall@5 & recall@10 & MAP \\
    \midrule
    \multirow{3}[0]{*}{single} & RNN   & 0.1283  & 0.3410  & 0.4397  & 0.2432  \\
          & RLBL  & 0.1389  & 0.3581  & 0.5277  & 0.2666  \\
          & TA-RLBL & 0.1227  & 0.3824  & 0.5221  & 0.2636  \\
    \midrule
    \multirow{2}[0]{*}{multiple} & RLBL  & \textbf{0.1600} & 0.3822  & \textbf{0.5519} & \textbf{0.2836} \\
          & TA-RLBL & 0.1413  & \textbf{0.4081} & 0.5461  & 0.2804  \\
    \bottomrule
    \end{tabular}%
  \label{tab:single_tmall}%
}
\label{tab:single}%
\end{table}

\subsection{Multiple Behaviors VS. Single Behavior}

To investigate the difference between multiple behaviors and single behavior, as same as for compared methods, we treat different behaviors towards one item as different elements in Movielens, GTD and Tmall for implementing RLBL and TA-RLBL. Thus, we can achieve their performances under single behavior. The performance comparison is shown in Table \ref{tab:single}. We can clearly observe the significant improvements brought by modeling multiple behaviors. Comparing with ignoring multiple behaviors, MAP improvements of RLBL are $2.92\%$, $10.52\%$ and $6.41\%$ on the three dataset respectively. And for TA-RLBL, the MAP improvements become $2.96\%$, $10.46\%$ and $6.42\%$, which are close to the previous ones. Moreover, we can also see that, even ignoring multiple behaviors, RLBL and TA-RLBL can still outperform RNN with a relatively significant advantage, which indicates the effectiveness of position-specific and time specific transition. Meanwhile, recall the results of RLBL and TA-RLBL with varying window width in Table \ref{tab:mine}, comparing with the results of RNN in Table \ref{tab:single}, even not with the best window width, most of the results of RLBL and TA-RLBL are still better than the performance of RNN.

\begin{table}[!tb]
\caption{Performance comparison with different behavioral history length.}
\centering\scriptsize
\subtable[Performance on the Movielens dataset.]{
    \begin{tabular}{cccrrrc}
    \toprule
    length & \multicolumn{2}{c}{method} & recall@1 & recall@5 & recall@10 & MAP \\
    \midrule
    \multirow{5}[0]{*}{short} & \multicolumn{2}{c}{FPMC} & \multicolumn{1}{c}{0.0052 } & \multicolumn{1}{c}{0.0250 } & \multicolumn{1}{c}{0.0433 } & 0.0325  \\
          & \multicolumn{2}{c}{HRM} & \multicolumn{1}{c}{0.0057 } & \multicolumn{1}{c}{0.0283 } & \multicolumn{1}{c}{0.0456 } & 0.0339  \\
          & \multicolumn{2}{c}{RNN} & \multicolumn{1}{c}{0.0062 } & \multicolumn{1}{c}{0.0313 } & \multicolumn{1}{c}{0.0478 } & 0.0357  \\
          & \multicolumn{2}{c}{RLBL} & \multicolumn{1}{c}{0.0070 } & \multicolumn{1}{c}{0.0351 } & \multicolumn{1}{c}{0.0535 } & 0.0391  \\
          & \multicolumn{2}{c}{TA-RLBL} & \multicolumn{1}{c}{\textbf{0.0071}} & \multicolumn{1}{c}{\textbf{0.0368}} & \multicolumn{1}{c}{\textbf{0.0550}} & \textbf{0.0400} \\
    \midrule
    \multirow{5}[0]{*}{medium} & \multicolumn{2}{c}{FPMC} & \multicolumn{1}{c}{0.0054 } & \multicolumn{1}{c}{0.0257 } & \multicolumn{1}{c}{0.0441 } & 0.0333  \\
          & \multicolumn{2}{c}{HRM} & \multicolumn{1}{c}{0.0060 } & \multicolumn{1}{c}{0.0290 } & \multicolumn{1}{c}{0.0464 } & 0.0346  \\
          & \multicolumn{2}{c}{RNN} & \multicolumn{1}{c}{0.0063 } & \multicolumn{1}{c}{0.0317 } & \multicolumn{1}{c}{0.0483 } & 0.0361  \\
          & \multicolumn{2}{c}{RLBL} & \multicolumn{1}{c}{0.0072 } & \multicolumn{1}{c}{0.0356 } & \multicolumn{1}{c}{0.0540 } & 0.0397  \\
          & \multicolumn{2}{c}{TA-RLBL} & \multicolumn{1}{c}{\textbf{0.0073}} & \multicolumn{1}{c}{\textbf{0.0373}} & \multicolumn{1}{c}{\textbf{0.0556}} & \textbf{0.0405} \\
    \midrule
    \multirow{5}[0]{*}{long} & \multicolumn{2}{c}{FPMC} & \multicolumn{1}{c}{0.0053 } & \multicolumn{1}{c}{0.0254 } & \multicolumn{1}{c}{0.0438 } & 0.0330  \\
          & \multicolumn{2}{c}{HRM} & \multicolumn{1}{c}{0.0059 } & \multicolumn{1}{c}{0.0287 } & \multicolumn{1}{c}{0.0460 } & 0.0344  \\
          & \multicolumn{2}{c}{RNN} & \multicolumn{1}{c}{0.0064 } & \multicolumn{1}{c}{0.0320 } & \multicolumn{1}{c}{0.0487 } & 0.0364  \\
          & \multicolumn{2}{c}{RLBL} & \multicolumn{1}{c}{0.0073 } & \multicolumn{1}{c}{0.0359 } & \multicolumn{1}{c}{0.0544 } & 0.0400  \\
          & \multicolumn{2}{c}{TA-RLBL} & \multicolumn{1}{c}{\textbf{0.0074}} & \multicolumn{1}{c}{\textbf{0.0377}} & \multicolumn{1}{c}{\textbf{0.0561}} & \textbf{0.0409} \\
    \bottomrule
    \end{tabular}%
  \label{tab:length_movie}%
}
\qquad
\subtable[Performance on GTD.]{
    \begin{tabular}{cccrrrc}
    \toprule
    length & \multicolumn{2}{c}{method} & recall@1 & recall@5 & recall@10 & MAP \\
    \midrule
    \multirow{5}[0]{*}{short} & \multicolumn{2}{c}{FPMC} & \multicolumn{1}{c}{0.0935 } & \multicolumn{1}{c}{0.3834 } & \multicolumn{1}{c}{0.5658 } & 0.2341  \\
          & \multicolumn{2}{c}{HRM} & \multicolumn{1}{c}{0.0966 } & \multicolumn{1}{c}{0.3980 } & \multicolumn{1}{c}{0.5725 } & 0.2410  \\
          & \multicolumn{2}{c}{RNN} & \multicolumn{1}{c}{0.1180 } & \multicolumn{1}{c}{0.4066 } & \multicolumn{1}{c}{0.5833 } & 0.2544  \\
          & \multicolumn{2}{c}{RLBL} & \multicolumn{1}{c}{0.1507 } & \multicolumn{1}{c}{0.4728 } & \multicolumn{1}{c}{\textbf{0.6639}} & 0.3073  \\
          & \multicolumn{2}{c}{TA-RLBL} & \multicolumn{1}{c}{\textbf{0.1557}} & \multicolumn{1}{c}{\textbf{0.4770}} & \multicolumn{1}{c}{0.6621 } & \textbf{0.3124} \\
    \midrule
    \multirow{5}[0]{*}{medium} & \multicolumn{2}{c}{FPMC} & \multicolumn{1}{c}{0.0981 } & \multicolumn{1}{c}{0.4006 } & \multicolumn{1}{c}{0.5748 } & 0.2422  \\
          & \multicolumn{2}{c}{HRM} & \multicolumn{1}{c}{0.1029 } & \multicolumn{1}{c}{0.4124 } & \multicolumn{1}{c}{0.5830 } & 0.2503  \\
          & \multicolumn{2}{c}{RNN} & \multicolumn{1}{c}{0.1216 } & \multicolumn{1}{c}{0.4168 } & \multicolumn{1}{c}{0.5912 } & 0.2600  \\
          & \multicolumn{2}{c}{RLBL} & \multicolumn{1}{c}{0.1567 } & \multicolumn{1}{c}{0.4840 } & \multicolumn{1}{c}{\textbf{0.6734}} & 0.3140  \\
          & \multicolumn{2}{c}{TA-RLBL} & \multicolumn{1}{c}{\textbf{0.1620}} & \multicolumn{1}{c}{\textbf{0.4906}} & \multicolumn{1}{c}{0.6710 } & \textbf{0.3183} \\
    \midrule
    \multirow{5}[0]{*}{long} & \multicolumn{2}{c}{FPMC} & \multicolumn{1}{c}{0.0964 } & \multicolumn{1}{c}{0.3944 } & \multicolumn{1}{c}{0.5741 } & 0.2385  \\
          & \multicolumn{2}{c}{HRM} & \multicolumn{1}{c}{0.1007 } & \multicolumn{1}{c}{0.4068 } & \multicolumn{1}{c}{0.5824 } & 0.2468  \\
          & \multicolumn{2}{c}{RNN} & \multicolumn{1}{c}{0.1239 } & \multicolumn{1}{c}{0.4233 } & \multicolumn{1}{c}{0.5918 } & 0.2642  \\
          & \multicolumn{2}{c}{RLBL} & \multicolumn{1}{c}{0.1599 } & \multicolumn{1}{c}{0.4916 } & \multicolumn{1}{c}{\textbf{0.6790}} & 0.3178  \\
          & \multicolumn{2}{c}{TA-RLBL} & \multicolumn{1}{c}{\textbf{0.1649}} & \multicolumn{1}{c}{\textbf{0.4985}} & \multicolumn{1}{c}{0.6766 } & \textbf{0.3227} \\
    \bottomrule
    \end{tabular}%
  \label{tab:length_GTD}%
}
\qquad
\subtable[Performance on the Tmall dataset.]{
    \begin{tabular}{cccrrrc}
    \toprule
    length & \multicolumn{2}{c}{method} & recall@1 & recall@5 & recall@10 & MAP \\
    \midrule
    \multirow{5}[0]{*}{short} & \multicolumn{2}{c}{FPMC} & \multicolumn{1}{c}{0.1046 } & \multicolumn{1}{c}{0.2648 } & \multicolumn{1}{c}{0.3896 } & 0.2126  \\
          & \multicolumn{2}{c}{HRM} & \multicolumn{1}{c}{0.1168 } & \multicolumn{1}{c}{0.2941 } & \multicolumn{1}{c}{0.4265 } & 0.2341  \\
          & \multicolumn{2}{c}{RNN} & \multicolumn{1}{c}{0.1251 } & \multicolumn{1}{c}{0.3363 } & \multicolumn{1}{c}{0.4350 } & 0.2401  \\
          & \multicolumn{2}{c}{RLBL} & \multicolumn{1}{c}{\textbf{0.1566}} & \multicolumn{1}{c}{0.3786 } & \multicolumn{1}{c}{\textbf{0.5494}} & \textbf{0.2811} \\
          & \multicolumn{2}{c}{TA-RLBL} & \multicolumn{1}{c}{0.1381 } & \multicolumn{1}{c}{\textbf{0.4041}} & \multicolumn{1}{c}{0.5432 } & 0.2780  \\
    \midrule
    \multirow{5}[0]{*}{medium} & \multicolumn{2}{c}{FPMC} & \multicolumn{1}{c}{0.1090 } & \multicolumn{1}{c}{0.2719 } & \multicolumn{1}{c}{0.3968 } & 0.2174  \\
          & \multicolumn{2}{c}{HRM} & \multicolumn{1}{c}{0.1214 } & \multicolumn{1}{c}{0.3015 } & \multicolumn{1}{c}{0.4342 } & 0.2391  \\
          & \multicolumn{2}{c}{RNN} & \multicolumn{1}{c}{0.1282 } & \multicolumn{1}{c}{0.3410 } & \multicolumn{1}{c}{0.4396 } & 0.2432  \\
          & \multicolumn{2}{c}{RLBL} & \multicolumn{1}{c}{\textbf{0.1608}} & \multicolumn{1}{c}{0.3841 } & \multicolumn{1}{c}{\textbf{0.5547}} & \textbf{0.2850} \\
          & \multicolumn{2}{c}{TA-RLBL} & \multicolumn{1}{c}{0.1420 } & \multicolumn{1}{c}{\textbf{0.4102}} & \multicolumn{1}{c}{0.5491 } & 0.2818  \\
    \midrule
    \multirow{5}[0]{*}{long} & \multicolumn{2}{c}{FPMC} & \multicolumn{1}{c}{0.1074 } & \multicolumn{1}{c}{0.2692 } & \multicolumn{1}{c}{0.3939 } & 0.2154  \\
          & \multicolumn{2}{c}{HRM} & \multicolumn{1}{c}{0.1196 } & \multicolumn{1}{c}{0.2986 } & \multicolumn{1}{c}{0.4306 } & 0.2371  \\
          & \multicolumn{2}{c}{RNN} & \multicolumn{1}{c}{0.1303 } & \multicolumn{1}{c}{0.3445 } & \multicolumn{1}{c}{0.4433 } & 0.2447  \\
          & \multicolumn{2}{c}{RLBL} & \multicolumn{1}{c}{\textbf{0.1633}} & \multicolumn{1}{c}{0.3879 } & \multicolumn{1}{c}{\textbf{0.5588}} & \textbf{0.2876} \\
          & \multicolumn{2}{c}{TA-RLBL} & \multicolumn{1}{c}{0.1439 } & \multicolumn{1}{c}{\textbf{0.4143}} & \multicolumn{1}{c}{0.5532 } & 0.2842  \\
    \bottomrule
    \end{tabular}%
  \label{tab:length_tmall}%
}
\label{tab:length}%
\end{table}

\subsection{Comparison with Different Behavioral History Length}

Similar to the strategy of \cite{wang2015learning}, we split behavior sequences into three different types according to their length: short, medium and long. Thus, we can investigate the performance of models in different situations. In our experiments, for roughly equal splitting of behavioral sequences, we set the thresholds for Movielens $50$ and $200$, the thresholds for GTD $50$ and $200$, and the thresholds for Tmall $100$ and $500$. The performance comparison is shown in Table \ref{tab:length}. From the results, we can see that RLBL and TA-RLBL performs better than compared methods, i.e., FPMC, HRM and RNN, with different behavioral history length. This shows the flexibility of our methods with variety behavioral history length. Moreover, FPMC and HRM have the best performances on medium-length sequences, followed by long-length sequences. For RNN, RLBL and TA-RLBL, the longer the sequences, the better the performance. This may because FPMC and HRM only model the most recent behaviors when making prediction, and models with recurrent structure can take the whole sequence into consideration. Thus, our RLBL and TA-RLBL can easily deal with the situation when the sequences are too long.

\section{Conclusions and Future Work}

In this paper, we have proposed two novel multi-behavioral sequential prediction methods, i.e. recurrent log-bilinear model and time-aware recurrent log-bilinear model. In RLBL, we incorporate position-specific transition matrix as well as a recurrent structure. With such an architecture, RLBL can well model both short- and long-term contexts in a historical sequence. Besides, to capture multiple types of behavior in behavioral sequences, behavior-specific matrix is designed and applied for each type of behavior. Then, we further extend the RLBL model and propose a time-aware recurrent log-bilinear model with time-specific transition matrices. Incorporating time difference information, TA-RLBL can further improves the performance of RLBL. The experimental results on four real datasets show that both RLBL and TA-RLBL outperforms the state-of-the-art sequential prediction models.

In the future, we can further investigate the following direction. In RLBL and TA-RLBL, transition matrices are the same for different users, which does not confirm to practical situations. So, we need to find a method to determine different transition matrices for different users or different user groups. Moreover, we didn't take items' features, e.g., categories, descriptions and images of items, into consideration. Thus, incorporating our RLBL and TA-RLBL with features of items may also be our next step.

\small
\bibliographystyle{abbrv}
\bibliography{sigproc}
\vspace{2cm}

\balancecolumns
\end{document}
